# fluentd/conf/fluent.conf
<source>
  @type forward
  port "#{ENV['LOGGING_FLUENTD_PORT']}"
  bind 0.0.0.0
  max_retry_wait 3600
</source>

#
# Set default values to prevent errors
# Need this because of ignore_key_not_exist and suppress_parse_error_log do not work
# https://github.com/fluent/fluentd/issues/1617
#
<filter service.**>
  @type record_transformer
  <record>
    kvp ""
    message ""
  </record>
</filter>

#
# First-level filters to parse basic structure
#
<filter service.go.**>
  @type parser
  key_name log
  reserve_data yes
  format json
  suppress_parse_error_log true
  emit_invalid_record_to_error false
</filter>

<filter service.java.**>
  @type parser
  key_name log
  reserve_data yes
  <parse>
    @type grok
    grok_pattern ^%{TIMESTAMP_ISO8601:datetime}\s+%{LOGLEVEL:level}\s+\w+[\s-]+\[[^\]]+\]\s+%{NOTSPACE:class}\s+:\s+%{GREEDYDATA:message}
    time_format %Y-%m-%d %H:%M:%S
  </parse>
</filter>

<filter service.cpp.**>
  @type parser
  key_name log
  reserve_data yes
  <parse>
    @type grok
    grok_pattern ^%{TIME:time}\s+\[%{LOGLEVEL:level}\]\s+\[%{DATA:class}\]\s+%{GREEDYDATA:message}
  </parse>
</filter>


#
# Put `level`  into one case
#
<filter service.**>
  @type record_transformer
  enable_ruby
  <record>
    level ${record.key?("level") ? record["level"].upcase : ""}
  </record>
</filter>


#
# Parse stack and service from tag
#
<filter service.*.*.*.**>
  @type record_transformer
  enable_ruby
  <record>
    @stack ${tag.split(".")[2]}
    @service ${tag.split(".")[3].upcase}
  </record>
</filter>

#
# Key-value parsing
#
<filter service.**>
  @type parser
  key_name message
  reserve_data yes
  <parse>
    @type grok
    grok_pattern kv=\((?<kvp>[^\)]+)\)
  </parse>
</filter>
<filter service.**>
  @type parser
  key_name kvp
  reserve_data yes
  <parse>
    @type kv
    types count:integer,runtime:float
    kv_delimiter /[\s,]+/
  </parse>
</filter>


<match *.**>
  @type copy
  <store>
    @type elasticsearch
    host "#{ENV['LOGGING_ELASTICSEARCH_HOST']}"
    port "#{ENV['LOGGING_ELASTICSEARCH_PORT']}"
    logstash_format true
    logstash_prefix fluentd
    logstash_dateformat %Y%m%d
    include_tag_key true
    type_name docker_log
    tag_key @log_name
    flush_interval 1s
    reload_connections false
    reconnect_on_error true
  </store>
  <store>
    @type stdout
  </store>
</match>


#
# Prometheus
#
# count number of incoming records per tag
<filter service.**>
  @type prometheus
  <metric>
    name fluentd_input_status_num_records_total
    type counter
    desc The total number of incoming records
    <labels>
      tag ${tag}
      hostname ${hostname}
    </labels>
  </metric>
  </filter>

# count number of outgoing records per tag
<match *.**>
  @type copy
  <store>
    @type forward
    <server>
      name fluentd_prometheus_metrics
      host localhost
      port "#{ENV['LOGGING_FLUENTD_PORT']}"
      weight 60
    </server>
  </store>
  <store>
    @type prometheus
    <metric>
      name fluentd_output_status_num_records_total
      type counter
      desc The total number of outgoing records
      <labels>
        tag ${tag}
        hostname ${hostname}
      </labels>
    </metric>
  </store>
</match>

# expose metrics in prometheus format
<source>
  @type prometheus
  bind 0.0.0.0
  port "#{ENV['MONITORING_FLUENTD_PORT']}"
  metrics_path /metrics
</source>
<source>
  @type prometheus_output_monitor
  interval 10
  <labels>
    hostname ${hostname}
  </labels>
</source>